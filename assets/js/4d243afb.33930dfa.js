"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[6917],{28453:(e,n,s)=>{s.d(n,{R:()=>c,x:()=>o});var r=s(96540);const t={},i=r.createContext(t);function c(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),r.createElement(i.Provider,{value:n},e.children)}},80613:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>a,frontMatter:()=>c,metadata:()=>r,toc:()=>h});const r=JSON.parse('{"id":"api-specification/SpeechConverter/classes/SpeechConverter","title":"SpeechConverter","description":"aac-voice-api","source":"@site/docs/api-specification/SpeechConverter/classes/SpeechConverter.md","sourceDirName":"api-specification/SpeechConverter/classes","slug":"/api-specification/SpeechConverter/classes/SpeechConverter","permalink":"/project-001-aac-api/docs/api-specification/SpeechConverter/classes/SpeechConverter","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/edit/main/documentation/docs/api-specification/SpeechConverter/classes/SpeechConverter.md","tags":[],"version":"current","lastUpdatedBy":"Gino Russo","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"SpeechConverter","permalink":"/project-001-aac-api/docs/api-specification/SpeechConverter/"},"next":{"title":"transcribedLogEntry","permalink":"/project-001-aac-api/docs/api-specification/SpeechConverter/interfaces/transcribedLogEntry"}}');var t=s(74848),i=s(28453);const c={},o=void 0,d={},h=[{value:"Constructors",id:"constructors",level:2},{value:"Constructor",id:"constructor",level:3},{value:"Returns",id:"returns",level:4},{value:"Methods",id:"methods",level:2},{value:"getLoggedText()",id:"getloggedtext",level:3},{value:"Returns",id:"returns-1",level:4},{value:"getStatus()",id:"getstatus",level:3},{value:"Returns",id:"returns-2",level:4},{value:"getTranscribed()",id:"gettranscribed",level:3},{value:"Returns",id:"returns-3",level:4},{value:"Throws",id:"throws",level:4},{value:"init()",id:"init",level:3},{value:"Parameters",id:"parameters",level:4},{value:"Returns",id:"returns-4",level:4},{value:"startListening()",id:"startlistening",level:3},{value:"Returns",id:"returns-5",level:4},{value:"Throws",id:"throws-1",level:4},{value:"stopListening()",id:"stoplistening",level:3},{value:"Returns",id:"returns-6",level:4},{value:"Throws",id:"throws-2",level:4}];function l(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/",children:(0,t.jsx)(n.strong,{children:"aac-voice-api"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h1,{id:"class-speechconverter",children:"Class: SpeechConverter"}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L23",children:"SpeechConverter.ts:23"})]}),"\n",(0,t.jsx)(n.p,{children:"SpeechConverter handles real-time speech-to-text conversion using the Whisper model.\nIt manages audio input, preprocessing, and transcription directly in the browser."}),"\n",(0,t.jsx)(n.h2,{id:"constructors",children:"Constructors"}),"\n",(0,t.jsx)(n.h3,{id:"constructor",children:"Constructor"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"new SpeechConverter"}),"(): ",(0,t.jsx)(n.code,{children:"SpeechConverter"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L34",children:"SpeechConverter.ts:34"})]}),"\n",(0,t.jsx)(n.h4,{id:"returns",children:"Returns"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"SpeechConverter"})}),"\n",(0,t.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,t.jsx)(n.h3,{id:"getloggedtext",children:"getLoggedText()"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"getLoggedText"}),"(): ",(0,t.jsx)(n.code,{children:"string"}),"[]"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L312",children:"SpeechConverter.ts:312"})]}),"\n",(0,t.jsx)(n.h4,{id:"returns-1",children:"Returns"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"string"}),"[]"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"getstatus",children:"getStatus()"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"getStatus"}),"(): ",(0,t.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L332",children:"SpeechConverter.ts:332"})]}),"\n",(0,t.jsx)(n.p,{children:"Retrieves the current status of the Whisper model."}),"\n",(0,t.jsxs)(n.p,{children:["Returns ",(0,t.jsx)(n.code,{children:'"loading"'})," if the model has not been initialized yet,\notherwise returns the status string provided by the Whisper backend."]}),"\n",(0,t.jsx)(n.h4,{id:"returns-2",children:"Returns"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"string"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The current operational status of the Whisper module."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"gettranscribed",children:"getTranscribed()"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"getTranscribed"}),"(): ",(0,t.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L266",children:"SpeechConverter.ts:266"})]}),"\n",(0,t.jsx)(n.p,{children:"Retrieves the latest transcription result from the Whisper model and logs it."}),"\n",(0,t.jsx)(n.p,{children:"This method calls the underlying Whisper API to obtain the most recently\ntranscribed text. If any text has been returned from whisper, it logs it."}),"\n",(0,t.jsx)(n.h4,{id:"returns-3",children:"Returns"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"string"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The transcribed text from the current audio chunk."}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"throws",children:"Throws"}),"\n",(0,t.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"init",children:"init()"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"init"}),"(",(0,t.jsx)(n.code,{children:"modelPath"}),", ",(0,t.jsx)(n.code,{children:"lang"}),"): ",(0,t.jsx)(n.code,{children:"Promise"}),"<",(0,t.jsx)(n.code,{children:"void"}),">"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L81",children:"SpeechConverter.ts:81"})]}),"\n",(0,t.jsx)(n.p,{children:"Initializes the Whisper module with the specified model and language."}),"\n",(0,t.jsx)(n.p,{children:"This method:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Creates the Whisper instance asynchronously."}),"\n",(0,t.jsx)(n.li,{children:"Loads the model file into the in-memory filesystem."}),"\n",(0,t.jsx)(n.li,{children:"Initializes Whisper with the model path and language code."}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"parameters",children:"Parameters"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"modelPath"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"string"})}),(0,t.jsx)(n.td,{children:"Path or URL to the Whisper model file."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"lang"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"string"})}),(0,t.jsx)(n.td,{children:"Language code (e.g., 'en') to configure the model."})]})]})]}),"\n",(0,t.jsx)(n.h4,{id:"returns-4",children:"Returns"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"Promise"}),"<",(0,t.jsx)(n.code,{children:"void"}),">"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Resolves when the Whisper module is fully initialized."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"startlistening",children:"startListening()"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"startListening"}),"(): ",(0,t.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L173",children:"SpeechConverter.ts:173"})]}),"\n",(0,t.jsx)(n.p,{children:"Starts listening to the user's microphone input, collects audio chunks,\nand feeds them into the Whisper model for transcription in real time."}),"\n",(0,t.jsxs)(n.p,{children:["The method continuously gathers small chunks from ",(0,t.jsx)(n.code,{children:"AudioInputHandler"}),",\ncombines them into fixed-size blocks, downsamples them to 16kHz (required by Whisper),\nand sends them to the model for inference."]}),"\n",(0,t.jsx)(n.h4,{id:"returns-5",children:"Returns"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"void"})}),"\n",(0,t.jsx)(n.h4,{id:"throws-1",children:"Throws"}),"\n",(0,t.jsxs)(n.p,{children:["Throws if ",(0,t.jsx)(n.code,{children:"init()"})," was not called before invoking this method."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"stoplistening",children:"stopListening()"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"stopListening"}),"(): ",(0,t.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Defined in: ",(0,t.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/681b1bef6f4d46f8f7614169d87f151ce783205a/src/SpeechConverter.ts#L229",children:"SpeechConverter.ts:229"})]}),"\n",(0,t.jsx)(n.p,{children:"Stops the audio input stream and halts the real-time transcription process."}),"\n",(0,t.jsxs)(n.p,{children:["This should be called after ",(0,t.jsx)(n.code,{children:"startListening()"})," to stop capturing microphone input\nand free up system audio resources."]}),"\n",(0,t.jsx)(n.h4,{id:"returns-6",children:"Returns"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"void"})}),"\n",(0,t.jsx)(n.h4,{id:"throws-2",children:"Throws"}),"\n",(0,t.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."})]})}function a(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);