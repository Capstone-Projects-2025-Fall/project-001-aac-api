"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[1591],{28453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>a});var t=i(96540);const r={},s=t.createContext(r);function c(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),t.createElement(s.Provider,{value:n},e.children)}},95297:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"api-specification/classes/SpeechConverterOffline","title":"SpeechConverterOffline","description":"aac-voice-api","source":"@site/docs/api-specification/classes/SpeechConverterOffline.md","sourceDirName":"api-specification/classes","slug":"/api-specification/classes/SpeechConverterOffline","permalink":"/project-001-aac-api/docs/api-specification/classes/SpeechConverterOffline","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/edit/main/documentation/docs/api-specification/classes/SpeechConverterOffline.md","tags":[],"version":"current","lastUpdatedBy":"Michael Colbert","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"CommandLibrary","permalink":"/project-001-aac-api/docs/api-specification/classes/CommandLibrary"},"next":{"title":"SpeechConverterOnline","permalink":"/project-001-aac-api/docs/api-specification/classes/SpeechConverterOnline"}}');var r=i(74848),s=i(28453);const c={},a=void 0,l={},o=[{value:"Implements",id:"implements",level:2},{value:"Constructors",id:"constructors",level:2},{value:"Constructor",id:"constructor",level:3},{value:"Returns",id:"returns",level:4},{value:"Methods",id:"methods",level:2},{value:"getStatus()",id:"getstatus",level:3},{value:"Returns",id:"returns-1",level:4},{value:"Implementation of",id:"implementation-of",level:4},{value:"getTextLog()",id:"gettextlog",level:3},{value:"Returns",id:"returns-2",level:4},{value:"Implementation of",id:"implementation-of-1",level:4},{value:"getTranscribed()",id:"gettranscribed",level:3},{value:"Returns",id:"returns-3",level:4},{value:"Throws",id:"throws",level:4},{value:"Implementation of",id:"implementation-of-2",level:4},{value:"init()",id:"init",level:3},{value:"Parameters",id:"parameters",level:4},{value:"Returns",id:"returns-4",level:4},{value:"Implementation of",id:"implementation-of-3",level:4},{value:"startListening()",id:"startlistening",level:3},{value:"Returns",id:"returns-5",level:4},{value:"Throws",id:"throws-1",level:4},{value:"Implementation of",id:"implementation-of-4",level:4},{value:"stopListening()",id:"stoplistening",level:3},{value:"Returns",id:"returns-6",level:4},{value:"Throws",id:"throws-2",level:4},{value:"Implementation of",id:"implementation-of-5",level:4}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/",children:(0,r.jsx)(n.strong,{children:"aac-voice-api"})})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L12",children:"SpeechConverterOffline.ts:12"})]}),"\n",(0,r.jsx)(n.p,{children:"SpeechConverter handles real-time speech-to-text conversion using the Whisper model.\nIt manages audio input, preprocessing, and transcription directly in the browser."}),"\n",(0,r.jsx)(n.h2,{id:"implements",children:"Implements"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface",children:(0,r.jsx)(n.code,{children:"SpeechConverterInterface"})})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"constructors",children:"Constructors"}),"\n",(0,r.jsx)(n.h3,{id:"constructor",children:"Constructor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ts",children:"new SpeechConverterOffline(): SpeechConverterOffline;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L23",children:"SpeechConverterOffline.ts:23"})]}),"\n",(0,r.jsx)(n.h4,{id:"returns",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"SpeechConverterOffline"})}),"\n",(0,r.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,r.jsx)(n.h3,{id:"getstatus",children:"getStatus()"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ts",children:"getStatus(): string;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L299",children:"SpeechConverterOffline.ts:299"})]}),"\n",(0,r.jsx)(n.p,{children:"Retrieves the current status of the Whisper model."}),"\n",(0,r.jsxs)(n.p,{children:["Returns ",(0,r.jsx)(n.code,{children:'"loading"'})," if the model has not been initialized yet,\notherwise returns the status string provided by the Whisper backend."]}),"\n",(0,r.jsx)(n.h4,{id:"returns-1",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"string"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The current operational status of the Whisper module."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"implementation-of",children:"Implementation of"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface",children:(0,r.jsx)(n.code,{children:"SpeechConverterInterface"})}),".",(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface#getstatus",children:(0,r.jsx)(n.code,{children:"getStatus"})})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"gettextlog",children:"getTextLog()"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ts",children:"getTextLog(): string[];\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L280",children:"SpeechConverterOffline.ts:280"})]}),"\n",(0,r.jsx)(n.h4,{id:"returns-2",children:"Returns"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"string"}),"[]"]}),"\n",(0,r.jsx)(n.h4,{id:"implementation-of-1",children:"Implementation of"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface",children:(0,r.jsx)(n.code,{children:"SpeechConverterInterface"})}),".",(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface#gettextlog",children:(0,r.jsx)(n.code,{children:"getTextLog"})})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"gettranscribed",children:"getTranscribed()"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ts",children:"getTranscribed(): string;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L234",children:"SpeechConverterOffline.ts:234"})]}),"\n",(0,r.jsx)(n.p,{children:"Retrieves the latest transcription result from the Whisper model and logs it."}),"\n",(0,r.jsx)(n.p,{children:"This method calls the underlying Whisper API to obtain the most recently\ntranscribed text. If any text has been returned from whisper, it logs it."}),"\n",(0,r.jsx)(n.h4,{id:"returns-3",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"string"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The transcribed text from the current audio chunk."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"throws",children:"Throws"}),"\n",(0,r.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,r.jsx)(n.h4,{id:"implementation-of-2",children:"Implementation of"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface",children:(0,r.jsx)(n.code,{children:"SpeechConverterInterface"})}),".",(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface#gettranscribed",children:(0,r.jsx)(n.code,{children:"getTranscribed"})})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"init",children:"init()"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ts",children:"init(modelPath, lang): Promise<void>;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L66",children:"SpeechConverterOffline.ts:66"})]}),"\n",(0,r.jsx)(n.p,{children:"Initializes the Whisper module with the specified model and language."}),"\n",(0,r.jsx)(n.p,{children:"This method:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Creates the Whisper instance asynchronously."}),"\n",(0,r.jsx)(n.li,{children:"Loads the model file into the in-memory filesystem."}),"\n",(0,r.jsx)(n.li,{children:"Initializes Whisper with the model path and language code."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"parameters",children:"Parameters"}),"\n",(0,r.jsxs)("table",{children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Parameter"}),(0,r.jsx)("th",{children:"Type"}),(0,r.jsx)("th",{children:"Description"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"modelPath"})})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"string"})})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Path or URL to the Whisper model file."})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"lang"})})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"string"})})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Language code (e.g., 'en') to configure the model."})})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"returns-4",children:"Returns"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"Promise"}),"<",(0,r.jsx)(n.code,{children:"void"}),">"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Resolves when the Whisper module is fully initialized."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"implementation-of-3",children:"Implementation of"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface",children:(0,r.jsx)(n.code,{children:"SpeechConverterInterface"})}),".",(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface#init",children:(0,r.jsx)(n.code,{children:"init"})})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"startlistening",children:"startListening()"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ts",children:"startListening(): void;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L152",children:"SpeechConverterOffline.ts:152"})]}),"\n",(0,r.jsx)(n.p,{children:"Starts listening to the user's microphone input, collects audio chunks,\nand feeds them into the Whisper model for transcription in real time."}),"\n",(0,r.jsxs)(n.p,{children:["The method continuously gathers small chunks from ",(0,r.jsx)(n.code,{children:"AudioInputHandler"}),",\ncombines them into fixed-size blocks, downsamples them to 16kHz (required by Whisper),\nand sends them to the model for inference."]}),"\n",(0,r.jsx)(n.h4,{id:"returns-5",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"void"})}),"\n",(0,r.jsx)(n.h4,{id:"throws-1",children:"Throws"}),"\n",(0,r.jsxs)(n.p,{children:["Throws if ",(0,r.jsx)(n.code,{children:"init()"})," was not called before invoking this method."]}),"\n",(0,r.jsx)(n.h4,{id:"implementation-of-4",children:"Implementation of"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface",children:(0,r.jsx)(n.code,{children:"SpeechConverterInterface"})}),".",(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface#startlistening",children:(0,r.jsx)(n.code,{children:"startListening"})})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"stoplistening",children:"stopListening()"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ts",children:"stopListening(): void;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/8ef19aa1ae76b73971ed5e6f21dc113d3ead9745/src/SpeechConverterOffline.ts#L199",children:"SpeechConverterOffline.ts:199"})]}),"\n",(0,r.jsx)(n.p,{children:"Stops the audio input stream and halts the real-time transcription process."}),"\n",(0,r.jsxs)(n.p,{children:["This should be called after ",(0,r.jsx)(n.code,{children:"startListening()"})," to stop capturing microphone input\nand free up system audio resources."]}),"\n",(0,r.jsx)(n.h4,{id:"returns-6",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"void"})}),"\n",(0,r.jsx)(n.h4,{id:"throws-2",children:"Throws"}),"\n",(0,r.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,r.jsx)(n.h4,{id:"implementation-of-5",children:"Implementation of"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface",children:(0,r.jsx)(n.code,{children:"SpeechConverterInterface"})}),".",(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/interfaces/SpeechConverterInterface#stoplistening",children:(0,r.jsx)(n.code,{children:"stopListening"})})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);