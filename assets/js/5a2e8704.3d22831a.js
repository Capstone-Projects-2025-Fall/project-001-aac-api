"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[2237],{28453:(e,n,r)=>{r.d(n,{R:()=>d,x:()=>l});var s=r(96540);const i={},t=s.createContext(i);function d(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),s.createElement(t.Provider,{value:n},e.children)}},36576:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>a,frontMatter:()=>d,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"api-specification/SpeechConverter","title":"SpeechConverter","description":"project-001-aac-api","source":"@site/docs/api-specification/SpeechConverter.md","sourceDirName":"api-specification","slug":"/api-specification/SpeechConverter","permalink":"/project-001-aac-api/docs/api-specification/SpeechConverter","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/edit/main/documentation/docs/api-specification/SpeechConverter.md","tags":[],"version":"current","lastUpdatedBy":"Jess","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"CommandHistory","permalink":"/project-001-aac-api/docs/api-specification/CommandHistory"},"next":{"title":"index","permalink":"/project-001-aac-api/docs/api-specification/"}}');var i=r(74848),t=r(28453);const d={},l=void 0,c={},o=[{value:"Constructors",id:"constructors",level:2},{value:"Constructor",id:"constructor",level:3},{value:"Returns",id:"returns",level:4},{value:"Properties",id:"properties",level:2},{value:"audioHandler",id:"audiohandler",level:3},{value:"transcribedText",id:"transcribedtext",level:3},{value:"whisper",id:"whisper",level:3},{value:"Methods",id:"methods",level:2},{value:"combineChunks()",id:"combinechunks",level:3},{value:"Parameters",id:"parameters",level:4},{value:"buffer",id:"buffer",level:5},{value:"blockSize",id:"blocksize",level:5},{value:"Returns",id:"returns-1",level:4},{value:"downSample()",id:"downsample",level:3},{value:"Parameters",id:"parameters-1",level:4},{value:"input",id:"input",level:5},{value:"inputRate",id:"inputrate",level:5},{value:"outputRate",id:"outputrate",level:5},{value:"Returns",id:"returns-2",level:4},{value:"getStatus()",id:"getstatus",level:3},{value:"Returns",id:"returns-3",level:4},{value:"getTranscribed()",id:"gettranscribed",level:3},{value:"Returns",id:"returns-4",level:4},{value:"Throws",id:"throws",level:4},{value:"init()",id:"init",level:3},{value:"Parameters",id:"parameters-2",level:4},{value:"modelPath",id:"modelpath",level:5},{value:"lang",id:"lang",level:5},{value:"Returns",id:"returns-5",level:4},{value:"loadModelToFS()",id:"loadmodeltofs",level:3},{value:"Parameters",id:"parameters-3",level:4},{value:"modelPath",id:"modelpath-1",level:5},{value:"Returns",id:"returns-6",level:4},{value:"Throws",id:"throws-1",level:4},{value:"setAudio()",id:"setaudio",level:3},{value:"Parameters",id:"parameters-4",level:4},{value:"index",id:"index",level:5},{value:"audio",id:"audio",level:5},{value:"Returns",id:"returns-7",level:4},{value:"Throws",id:"throws-2",level:4},{value:"startListening()",id:"startlistening",level:3},{value:"Returns",id:"returns-8",level:4},{value:"Throws",id:"throws-3",level:4},{value:"stopListening()",id:"stoplistening",level:3},{value:"Returns",id:"returns-9",level:4},{value:"Throws",id:"throws-4",level:4}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"../../README.md",children:(0,i.jsx)(n.strong,{children:"project-001-aac-api"})})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L13",children:"SpeechConverter.ts:13"})]}),"\n",(0,i.jsx)(n.p,{children:"SpeechConverter handles real-time speech-to-text conversion using the Whisper model.\nIt manages audio input, preprocessing, and transcription directly in the browser."}),"\n",(0,i.jsx)(n.h2,{id:"constructors",children:"Constructors"}),"\n",(0,i.jsx)(n.h3,{id:"constructor",children:"Constructor"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"new SpeechConverter"}),"(): ",(0,i.jsx)(n.code,{children:"SpeechConverter"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L21",children:"SpeechConverter.ts:21"})]}),"\n",(0,i.jsx)(n.h4,{id:"returns",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"SpeechConverter"})}),"\n",(0,i.jsx)(n.h2,{id:"properties",children:"Properties"}),"\n",(0,i.jsx)(n.h3,{id:"audiohandler",children:"audioHandler"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"audioHandler"}),": ",(0,i.jsx)(n.code,{children:"null"})," | ",(0,i.jsx)(n.a,{href:"../../AudioInputHandler/classes/AudioInputHandler.md",children:(0,i.jsx)(n.code,{children:"AudioInputHandler"})})," = ",(0,i.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L17",children:"SpeechConverter.ts:17"})]}),"\n",(0,i.jsx)(n.p,{children:"Used to capture microphone input"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"transcribedtext",children:"transcribedText"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"transcribedText"}),": ",(0,i.jsx)(n.code,{children:"null"})," | ",(0,i.jsx)(n.a,{href:"../../CommandHistory/classes/CommandHistory.md",children:(0,i.jsx)(n.code,{children:"CommandHistory"})})," = ",(0,i.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L19",children:"SpeechConverter.ts:19"})]}),"\n",(0,i.jsx)(n.p,{children:"Stores all transcribed text segments captured from audio."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"whisper",children:"whisper"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"whisper"}),": ",(0,i.jsx)(n.code,{children:"null"})," | ",(0,i.jsx)(n.a,{href:"../../whisper/libstream/interfaces/WhisperModule.md",children:(0,i.jsx)(n.code,{children:"WhisperModule"})})," = ",(0,i.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L15",children:"SpeechConverter.ts:15"})]}),"\n",(0,i.jsx)(n.p,{children:"Reference to the WhisperModule instance for transcribing data"}),"\n",(0,i.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,i.jsx)(n.h3,{id:"combinechunks",children:"combineChunks()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"combineChunks"}),"(",(0,i.jsx)(n.code,{children:"buffer"}),", ",(0,i.jsx)(n.code,{children:"blockSize"}),"): ",(0,i.jsx)(n.code,{children:"Float32Array"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L122",children:"SpeechConverter.ts:122"})]}),"\n",(0,i.jsx)(n.p,{children:"Combines multiple smaller Float32Array chunks into a single fixed-size block.\nIf the combined length is less than oneBlockSamples, it fills up using chunks\nsequentially from the buffer until the block is full or the buffer is empty."}),"\n",(0,i.jsx)(n.h4,{id:"parameters",children:"Parameters"}),"\n",(0,i.jsx)(n.h5,{id:"buffer",children:"buffer"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Float32Array"}),"<",(0,i.jsx)(n.code,{children:"ArrayBufferLike"}),">[]"]}),"\n",(0,i.jsx)(n.p,{children:"Array of audio data chunks waiting to be combined"}),"\n",(0,i.jsx)(n.h5,{id:"blocksize",children:"blockSize"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"Total length of Array that will be returned"}),"\n",(0,i.jsx)(n.h4,{id:"returns-1",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,i.jsx)(n.p,{children:"Returns a single Array of size block size"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"downsample",children:"downSample()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"downSample"}),"(",(0,i.jsx)(n.code,{children:"input"}),", ",(0,i.jsx)(n.code,{children:"inputRate"}),", ",(0,i.jsx)(n.code,{children:"outputRate"}),"): ",(0,i.jsx)(n.code,{children:"Float32Array"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L84",children:"SpeechConverter.ts:84"})]}),"\n",(0,i.jsx)(n.p,{children:"Takes an input Float32Array and downsamples the data to the given output rate provided"}),"\n",(0,i.jsx)(n.h4,{id:"parameters-1",children:"Parameters"}),"\n",(0,i.jsx)(n.h5,{id:"input",children:"input"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,i.jsx)(n.p,{children:"Audio sample to be downsampled"}),"\n",(0,i.jsx)(n.h5,{id:"inputrate",children:"inputRate"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"The sample rate that the audio was recorded in"}),"\n",(0,i.jsx)(n.h5,{id:"outputrate",children:"outputRate"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"The sample rate the audio is being downsampled to"}),"\n",(0,i.jsx)(n.h4,{id:"returns-2",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,i.jsx)(n.p,{children:"The down sampled data in a Float32Array object"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"getstatus",children:"getStatus()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"getStatus"}),"(): ",(0,i.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L262",children:"SpeechConverter.ts:262"})]}),"\n",(0,i.jsx)(n.p,{children:"Retrieves the current status of the Whisper model."}),"\n",(0,i.jsxs)(n.p,{children:["Returns ",(0,i.jsx)(n.code,{children:'"loading"'})," if the model has not been initialized yet,\notherwise returns the status string provided by the Whisper backend."]}),"\n",(0,i.jsx)(n.h4,{id:"returns-3",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The current operational status of the Whisper module."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"gettranscribed",children:"getTranscribed()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"getTranscribed"}),"(): ",(0,i.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L244",children:"SpeechConverter.ts:244"})]}),"\n",(0,i.jsx)(n.p,{children:"Retrieves the latest transcription result from the Whisper model and stores it."}),"\n",(0,i.jsx)(n.p,{children:"This method calls the underlying Whisper API to obtain the most recently\ntranscribed text, appends it to the internal transcript list, and returns\nthe accumulated transcription history."}),"\n",(0,i.jsx)(n.h4,{id:"returns-4",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"void"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"An array containing all transcribed text segments so far."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"throws",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"init",children:"init()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"init"}),"(",(0,i.jsx)(n.code,{children:"modelPath"}),", ",(0,i.jsx)(n.code,{children:"lang"}),"): ",(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"void"}),">"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L68",children:"SpeechConverter.ts:68"})]}),"\n",(0,i.jsx)(n.p,{children:"Initializes the Whisper module with the specified model and language."}),"\n",(0,i.jsx)(n.p,{children:"This method:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Creates the Whisper instance asynchronously."}),"\n",(0,i.jsx)(n.li,{children:"Loads the model file into the in-memory filesystem."}),"\n",(0,i.jsx)(n.li,{children:"Initializes Whisper with the model path and language code."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"parameters-2",children:"Parameters"}),"\n",(0,i.jsx)(n.h5,{id:"modelpath",children:"modelPath"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"Path or URL to the Whisper model file."}),"\n",(0,i.jsx)(n.h5,{id:"lang",children:"lang"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"Language code (e.g., 'en') to configure the model."}),"\n",(0,i.jsx)(n.h4,{id:"returns-5",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"void"}),">"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Resolves when the Whisper module is fully initialized."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"loadmodeltofs",children:"loadModelToFS()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"loadModelToFS"}),"(",(0,i.jsx)(n.code,{children:"modelPath"}),"): ",(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"string"}),">"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L37",children:"SpeechConverter.ts:37"})]}),"\n",(0,i.jsx)(n.p,{children:"Loads a Whisper model from a given path (local or remote) into the in-memory file system."}),"\n",(0,i.jsx)(n.p,{children:"Fetches the model file, writes it into the MEMFS, and returns the internal path\nwhere the model is stored for later use by the Whisper instance."}),"\n",(0,i.jsx)(n.h4,{id:"parameters-3",children:"Parameters"}),"\n",(0,i.jsx)(n.h5,{id:"modelpath-1",children:"modelPath"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"The path or URL to the model file to load."}),"\n",(0,i.jsx)(n.h4,{id:"returns-6",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"string"}),">"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The internal file path in MEMFS where the model is stored."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"throws-1",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module is not initialized or fetch fails."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"setaudio",children:"setAudio()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"setAudio"}),"(",(0,i.jsx)(n.code,{children:"index"}),", ",(0,i.jsx)(n.code,{children:"audio"}),"): ",(0,i.jsx)(n.code,{children:"number"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L226",children:"SpeechConverter.ts:226"})]}),"\n",(0,i.jsx)(n.p,{children:"Sets the audio data at a given index for the Whisper model."}),"\n",(0,i.jsx)(n.p,{children:"This method passes a Float32Array of audio samples\ndirectly to the Whisper backend for processing."}),"\n",(0,i.jsx)(n.h4,{id:"parameters-4",children:"Parameters"}),"\n",(0,i.jsx)(n.h5,{id:"index",children:"index"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"The index position of the audio buffer to set (usually 0 or 1 for channels)."}),"\n",(0,i.jsx)(n.h5,{id:"audio",children:"audio"}),"\n",(0,i.jsx)(n.p,{children:"The raw audio data to be sent to the Whisper model."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Float32Array"}),"<",(0,i.jsx)(n.code,{children:"ArrayBufferLike"}),"> | ",(0,i.jsx)(n.code,{children:"number"}),"[]"]}),"\n",(0,i.jsx)(n.h4,{id:"returns-7",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The status code or result returned by the Whisper backend."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"throws-2",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"startlistening",children:"startListening()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"startListening"}),"(): ",(0,i.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L160",children:"SpeechConverter.ts:160"})]}),"\n",(0,i.jsx)(n.p,{children:"Starts listening to the user's microphone input, collects audio chunks,\nand feeds them into the Whisper model for transcription in real time."}),"\n",(0,i.jsxs)(n.p,{children:["The method continuously gathers small chunks from ",(0,i.jsx)(n.code,{children:"AudioInputHandler"}),",\ncombines them into fixed-size blocks, downsamples them to 16kHz (required by Whisper),\nand sends them to the model for inference."]}),"\n",(0,i.jsx)(n.h4,{id:"returns-8",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"void"})}),"\n",(0,i.jsx)(n.h4,{id:"throws-3",children:"Throws"}),"\n",(0,i.jsxs)(n.p,{children:["Throws if ",(0,i.jsx)(n.code,{children:"init()"})," was not called before invoking this method."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"stoplistening",children:"stopListening()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"stopListening"}),"(): ",(0,i.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/bc21471f230194581e4f0db915456df0e64b2367/src/SpeechConverter.ts#L206",children:"SpeechConverter.ts:206"})]}),"\n",(0,i.jsx)(n.p,{children:"Stops the audio input stream and halts the real-time transcription process."}),"\n",(0,i.jsxs)(n.p,{children:["This should be called after ",(0,i.jsx)(n.code,{children:"startListening()"})," to stop capturing microphone input\nand free up system audio resources."]}),"\n",(0,i.jsx)(n.h4,{id:"returns-9",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"void"})}),"\n",(0,i.jsx)(n.h4,{id:"throws-4",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."})]})}function a(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);