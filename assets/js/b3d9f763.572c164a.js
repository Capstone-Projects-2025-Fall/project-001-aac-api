"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[5691],{4576:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>c,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"API Specification/AudioInputHandler/README","title":"README","description":"aac-voice-api","source":"@site/docs/API Specification/AudioInputHandler/README.md","sourceDirName":"API Specification/AudioInputHandler","slug":"/API Specification/AudioInputHandler/","permalink":"/project-001-aac-api/docs/API Specification/AudioInputHandler/","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/edit/main/documentation/docs/API Specification/AudioInputHandler/README.md","tags":[],"version":"current","lastUpdatedBy":"Tam Trang","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"README","permalink":"/project-001-aac-api/docs/API Specification/AACVoiceAPI/"},"next":{"title":"README","permalink":"/project-001-aac-api/docs/API Specification/CommandConverter/"}}');var r=s(74848),t=s(28453);const c={},d=void 0,o={},l=[{value:"Classes",id:"classes",level:2},{value:"AudioInputHandler",id:"audioinputhandler",level:3},{value:"Constructors",id:"constructors",level:4},{value:"Constructor",id:"constructor",level:5},{value:"Parameters",id:"parameters",level:6},{value:"onAudioChunk",id:"onaudiochunk",level:6},{value:"Returns",id:"returns",level:6},{value:"Properties",id:"properties",level:4},{value:"ctx",id:"ctx",level:5},{value:"isListening",id:"islistening",level:5},{value:"onAudioChunk()",id:"onaudiochunk-1",level:5},{value:"Parameters",id:"parameters-1",level:6},{value:"chunk",id:"chunk",level:6},{value:"Returns",id:"returns-1",level:6},{value:"processor",id:"processor",level:5},{value:"stream",id:"stream",level:5},{value:"Methods",id:"methods",level:4},{value:"getSampleRate()",id:"getsamplerate",level:5},{value:"Returns",id:"returns-2",level:6},{value:"startListening()",id:"startlistening",level:5},{value:"Returns",id:"returns-3",level:6},{value:"stopListening()",id:"stoplistening",level:5},{value:"Returns",id:"returns-4",level:6}];function a(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"/project-001-aac-api/docs/API%20Specification/",children:(0,r.jsx)(n.strong,{children:"aac-voice-api"})})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"classes",children:"Classes"}),"\n",(0,r.jsx)(n.h3,{id:"audioinputhandler",children:"AudioInputHandler"}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L12",children:"AudioInputHandler.ts:12"})]}),"\n",(0,r.jsx)(n.p,{children:"AudioInputHandler is a microphone input handler that:\nCaptures audio from the user\u2019s microphone.\nProcesses audio in chunks (Float32Array).\nSends those chunks to a callback function for further processing.\nIt also provides start/stop control and exposes the audio sample rate."}),"\n",(0,r.jsx)(n.h4,{id:"constructors",children:"Constructors"}),"\n",(0,r.jsx)(n.h5,{id:"constructor",children:"Constructor"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"new AudioInputHandler"}),"(",(0,r.jsx)(n.code,{children:"onAudioChunk"}),"): ",(0,r.jsx)(n.a,{href:"#audioinputhandler",children:(0,r.jsx)(n.code,{children:"AudioInputHandler"})})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L33",children:"AudioInputHandler.ts:33"})]}),"\n",(0,r.jsx)(n.p,{children:"Creates a new AudioInputHandler."}),"\n",(0,r.jsx)(n.h6,{id:"parameters",children:"Parameters"}),"\n",(0,r.jsx)(n.h6,{id:"onaudiochunk",children:"onAudioChunk"}),"\n",(0,r.jsxs)(n.p,{children:["(",(0,r.jsx)(n.code,{children:"chunk"}),") => ",(0,r.jsx)(n.code,{children:"void"})]}),"\n",(0,r.jsx)(n.p,{children:"A callback function that is called whenever\nan audio chunk is captured. Receives a Float32Array\ncontaining the audio samples."}),"\n",(0,r.jsx)(n.h6,{id:"returns",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"#audioinputhandler",children:(0,r.jsx)(n.code,{children:"AudioInputHandler"})})}),"\n",(0,r.jsx)(n.h4,{id:"properties",children:"Properties"}),"\n",(0,r.jsx)(n.h5,{id:"ctx",children:"ctx"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"private"})," ",(0,r.jsx)(n.strong,{children:"ctx"}),": ",(0,r.jsx)(n.code,{children:"null"})," | ",(0,r.jsx)(n.code,{children:"AudioContext"})," = ",(0,r.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L17",children:"AudioInputHandler.ts:17"})]}),"\n",(0,r.jsx)(n.p,{children:"Used for processing the audio"}),"\n",(0,r.jsx)(n.h5,{id:"islistening",children:"isListening"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"isListening"}),": ",(0,r.jsx)(n.code,{children:"boolean"})," = ",(0,r.jsx)(n.code,{children:"false"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L21",children:"AudioInputHandler.ts:21"})]}),"\n",(0,r.jsx)(n.p,{children:"Flag that checks if startListening has already been called"}),"\n",(0,r.jsx)(n.h5,{id:"onaudiochunk-1",children:"onAudioChunk()"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"private"})," ",(0,r.jsx)(n.strong,{children:"onAudioChunk"}),": (",(0,r.jsx)(n.code,{children:"chunk"}),") => ",(0,r.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L23",children:"AudioInputHandler.ts:23"})]}),"\n",(0,r.jsx)(n.p,{children:"Callback function that receives each audio chunk captured from the microphone."}),"\n",(0,r.jsx)(n.h6,{id:"parameters-1",children:"Parameters"}),"\n",(0,r.jsx)(n.h6,{id:"chunk",children:"chunk"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,r.jsx)(n.h6,{id:"returns-1",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"void"})}),"\n",(0,r.jsx)(n.h5,{id:"processor",children:"processor"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"private"})," ",(0,r.jsx)(n.strong,{children:"processor"}),": ",(0,r.jsx)(n.code,{children:"null"})," | ",(0,r.jsx)(n.code,{children:"ScriptProcessorNode"})," = ",(0,r.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L19",children:"AudioInputHandler.ts:19"})]}),"\n",(0,r.jsx)(n.p,{children:"Used to buffer audio data"}),"\n",(0,r.jsx)(n.h5,{id:"stream",children:"stream"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"private"})," ",(0,r.jsx)(n.strong,{children:"stream"}),": ",(0,r.jsx)(n.code,{children:"null"})," | ",(0,r.jsx)(n.code,{children:"MediaStream"})," = ",(0,r.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L15",children:"AudioInputHandler.ts:15"})]}),"\n",(0,r.jsx)(n.p,{children:"Media stream from users microphone"}),"\n",(0,r.jsx)(n.h4,{id:"methods",children:"Methods"}),"\n",(0,r.jsx)(n.h5,{id:"getsamplerate",children:"getSampleRate()"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"getSampleRate"}),"(): ",(0,r.jsx)(n.code,{children:"number"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L43",children:"AudioInputHandler.ts:43"})]}),"\n",(0,r.jsx)(n.p,{children:"Returns the sample rate of the audio context."}),"\n",(0,r.jsx)(n.h6,{id:"returns-2",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"number"})}),"\n",(0,r.jsxs)(n.p,{children:["The sample rate in Hz, or ",(0,r.jsx)(n.code,{children:"undefined"})," if the audio context is not initialized."]}),"\n",(0,r.jsx)(n.h5,{id:"startlistening",children:"startListening()"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"startListening"}),"(): ",(0,r.jsx)(n.code,{children:"Promise"}),"<",(0,r.jsx)(n.code,{children:"void"}),">"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L57",children:"AudioInputHandler.ts:57"})]}),"\n",(0,r.jsx)(n.p,{children:"Starts capturing audio from the user's microphone."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Prompts the user for microphone permissions."}),"\n",(0,r.jsx)(n.li,{children:"Creates an AudioContext and a ScriptProcessorNode to process audio in chunks."}),"\n",(0,r.jsxs)(n.li,{children:["Calls the ",(0,r.jsx)(n.code,{children:"onAudioChunk"})," callback with a Float32Array for each audio buffer."]}),"\n",(0,r.jsx)(n.li,{children:"Handles errors such as permission denial or missing microphone hardware."}),"\n"]}),"\n",(0,r.jsx)(n.h6,{id:"returns-3",children:"Returns"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"Promise"}),"<",(0,r.jsx)(n.code,{children:"void"}),">"]}),"\n",(0,r.jsx)(n.p,{children:"A Promise that resolves when listening has started."}),"\n",(0,r.jsx)(n.h5,{id:"stoplistening",children:"stopListening()"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"stopListening"}),"(): ",(0,r.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Defined in: ",(0,r.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/2e181446a0955d6e69720fafcb5e1ba075e3f20f/src/AudioInputHandler.ts#L111",children:"AudioInputHandler.ts:111"})]}),"\n",(0,r.jsx)(n.p,{children:"Stops capturing audio from the microphone and cleans up resources."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Disconnects the ScriptProcessorNode from the audio graph."}),"\n",(0,r.jsx)(n.li,{children:"Closes the AudioContext."}),"\n",(0,r.jsx)(n.li,{children:"Stops all tracks of the MediaStream."}),"\n",(0,r.jsxs)(n.li,{children:["Updates the ",(0,r.jsx)(n.code,{children:"isListening"})," flag to ",(0,r.jsx)(n.code,{children:"false"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h6,{id:"returns-4",children:"Returns"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"void"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>c,x:()=>d});var i=s(96540);const r={},t=i.createContext(r);function c(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);