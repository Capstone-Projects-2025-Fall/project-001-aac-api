"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[8591],{28453:(e,n,r)=>{r.d(n,{R:()=>c,x:()=>l});var s=r(96540);const i={},t=s.createContext(i);function c(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),s.createElement(t.Provider,{value:n},e.children)}},66237:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>a,frontMatter:()=>c,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"api-specification/SpeechConverter/SpeechConverter","title":"SpeechConverter","description":"aac-voice-api","source":"@site/docs/api-specification/SpeechConverter/SpeechConverter.md","sourceDirName":"api-specification/SpeechConverter","slug":"/api-specification/SpeechConverter/","permalink":"/project-001-aac-api/docs/api-specification/SpeechConverter/","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/edit/main/documentation/docs/api-specification/SpeechConverter/SpeechConverter.md","tags":[],"version":"current","lastUpdatedBy":"Gino Russo","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"CommandHistory","permalink":"/project-001-aac-api/docs/api-specification/CommandHistory/"},"next":{"title":"commandLibrary","permalink":"/project-001-aac-api/docs/api-specification/commandLibrary/"}}');var i=r(74848),t=r(28453);const c={},l=void 0,d={},o=[{value:"Classes",id:"classes",level:2},{value:"SpeechConverter",id:"speechconverter",level:3},{value:"Constructors",id:"constructors",level:4},{value:"Constructor",id:"constructor",level:5},{value:"Returns",id:"returns",level:6},{value:"Properties",id:"properties",level:4},{value:"audioHandler",id:"audiohandler",level:5},{value:"commandConverter",id:"commandconverter",level:5},{value:"textLog",id:"textlog",level:5},{value:"transcriptionInterval?",id:"transcriptioninterval",level:5},{value:"whisper",id:"whisper",level:5},{value:"Methods",id:"methods",level:4},{value:"combineChunks()",id:"combinechunks",level:5},{value:"Parameters",id:"parameters",level:6},{value:"buffer",id:"buffer",level:6},{value:"blockSize",id:"blocksize",level:6},{value:"Returns",id:"returns-1",level:6},{value:"downSample()",id:"downsample",level:5},{value:"Parameters",id:"parameters-1",level:6},{value:"input",id:"input",level:6},{value:"inputRate",id:"inputrate",level:6},{value:"outputRate",id:"outputrate",level:6},{value:"Returns",id:"returns-2",level:6},{value:"getLoggedText()",id:"getloggedtext",level:5},{value:"Returns",id:"returns-3",level:6},{value:"getStatus()",id:"getstatus",level:5},{value:"Returns",id:"returns-4",level:6},{value:"getTranscribed()",id:"gettranscribed",level:5},{value:"Returns",id:"returns-5",level:6},{value:"Throws",id:"throws",level:6},{value:"init()",id:"init",level:5},{value:"Parameters",id:"parameters-2",level:6},{value:"modelPath",id:"modelpath",level:6},{value:"lang",id:"lang",level:6},{value:"Returns",id:"returns-6",level:6},{value:"loadModelToFS()",id:"loadmodeltofs",level:5},{value:"Parameters",id:"parameters-3",level:6},{value:"modelPath",id:"modelpath-1",level:6},{value:"Returns",id:"returns-7",level:6},{value:"Throws",id:"throws-1",level:6},{value:"logText()",id:"logtext",level:5},{value:"Parameters",id:"parameters-4",level:6},{value:"text",id:"text",level:6},{value:"Returns",id:"returns-8",level:6},{value:"processText()",id:"processtext",level:5},{value:"Parameters",id:"parameters-5",level:6},{value:"text",id:"text-1",level:6},{value:"Returns",id:"returns-9",level:6},{value:"setAudio()",id:"setaudio",level:5},{value:"Parameters",id:"parameters-6",level:6},{value:"index",id:"index",level:6},{value:"audio",id:"audio",level:6},{value:"Returns",id:"returns-10",level:6},{value:"Throws",id:"throws-2",level:6},{value:"startListening()",id:"startlistening",level:5},{value:"Returns",id:"returns-11",level:6},{value:"Throws",id:"throws-3",level:6},{value:"stopListening()",id:"stoplistening",level:5},{value:"Returns",id:"returns-12",level:6},{value:"Throws",id:"throws-4",level:6},{value:"Interfaces",id:"interfaces",level:2},{value:"transcribedLogEntry",id:"transcribedlogentry",level:3},{value:"Properties",id:"properties-1",level:4},{value:"timestamp",id:"timestamp",level:5},{value:"transcribedText",id:"transcribedtext",level:5}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/",children:(0,i.jsx)(n.strong,{children:"aac-voice-api"})})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"classes",children:"Classes"}),"\n",(0,i.jsx)(n.h3,{id:"speechconverter",children:"SpeechConverter"}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L23",children:"SpeechConverter.ts:23"})]}),"\n",(0,i.jsx)(n.p,{children:"SpeechConverter handles real-time speech-to-text conversion using the Whisper model.\nIt manages audio input, preprocessing, and transcription directly in the browser."}),"\n",(0,i.jsx)(n.h4,{id:"constructors",children:"Constructors"}),"\n",(0,i.jsx)(n.h5,{id:"constructor",children:"Constructor"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"new SpeechConverter"}),"(): ",(0,i.jsx)(n.a,{href:"#speechconverter",children:(0,i.jsx)(n.code,{children:"SpeechConverter"})})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L34",children:"SpeechConverter.ts:34"})]}),"\n",(0,i.jsx)(n.h6,{id:"returns",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"#speechconverter",children:(0,i.jsx)(n.code,{children:"SpeechConverter"})})}),"\n",(0,i.jsx)(n.h4,{id:"properties",children:"Properties"}),"\n",(0,i.jsx)(n.h5,{id:"audiohandler",children:"audioHandler"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"audioHandler"}),": ",(0,i.jsx)(n.code,{children:"null"})," | ",(0,i.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/AudioInputHandler/#audioinputhandler",children:(0,i.jsx)(n.code,{children:"AudioInputHandler"})})," = ",(0,i.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L27",children:"SpeechConverter.ts:27"})]}),"\n",(0,i.jsx)(n.p,{children:"Used to capture microphone input"}),"\n",(0,i.jsx)(n.h5,{id:"commandconverter",children:"commandConverter"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"commandConverter"}),": ",(0,i.jsx)(n.code,{children:"null"})," | ",(0,i.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/CommandConverter/#commandconverter",children:(0,i.jsx)(n.code,{children:"CommandConverter"})})," = ",(0,i.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L29",children:"SpeechConverter.ts:29"})]}),"\n",(0,i.jsx)(n.p,{children:"Processes transcribed text and matches commands"}),"\n",(0,i.jsx)(n.h5,{id:"textlog",children:"textLog"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"textLog"}),": ",(0,i.jsx)(n.code,{children:"null"})," | ",(0,i.jsx)(n.a,{href:"#transcribedlogentry",children:(0,i.jsx)(n.code,{children:"transcribedLogEntry"})}),"[] = ",(0,i.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L31",children:"SpeechConverter.ts:31"})]}),"\n",(0,i.jsx)(n.p,{children:"Keeps a log of all text that has been transcribed"}),"\n",(0,i.jsx)(n.h5,{id:"transcriptioninterval",children:"transcriptionInterval?"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.code,{children:"optional"})," ",(0,i.jsx)(n.strong,{children:"transcriptionInterval"}),": ",(0,i.jsx)(n.code,{children:"Timeout"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L32",children:"SpeechConverter.ts:32"})]}),"\n",(0,i.jsx)(n.h5,{id:"whisper",children:"whisper"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"whisper"}),": ",(0,i.jsx)(n.code,{children:"null"})," | ",(0,i.jsx)(n.a,{href:"/project-001-aac-api/docs/api-specification/whisper/libstream/#whispermodule",children:(0,i.jsx)(n.code,{children:"WhisperModule"})})," = ",(0,i.jsx)(n.code,{children:"null"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L25",children:"SpeechConverter.ts:25"})]}),"\n",(0,i.jsx)(n.p,{children:"Reference to the WhisperModule instance for transcribing data"}),"\n",(0,i.jsx)(n.h4,{id:"methods",children:"Methods"}),"\n",(0,i.jsx)(n.h5,{id:"combinechunks",children:"combineChunks()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"combineChunks"}),"(",(0,i.jsx)(n.code,{children:"buffer"}),", ",(0,i.jsx)(n.code,{children:"blockSize"}),"): ",(0,i.jsx)(n.code,{children:"Float32Array"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L135",children:"SpeechConverter.ts:135"})]}),"\n",(0,i.jsx)(n.p,{children:"Combines multiple smaller Float32Array chunks into a single fixed-size block.\nIf the combined length is less than oneBlockSamples, it fills up using chunks\nsequentially from the buffer until the block is full or the buffer is empty."}),"\n",(0,i.jsx)(n.h6,{id:"parameters",children:"Parameters"}),"\n",(0,i.jsx)(n.h6,{id:"buffer",children:"buffer"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Float32Array"}),"<",(0,i.jsx)(n.code,{children:"ArrayBufferLike"}),">[]"]}),"\n",(0,i.jsx)(n.p,{children:"Array of audio data chunks waiting to be combined"}),"\n",(0,i.jsx)(n.h6,{id:"blocksize",children:"blockSize"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"Total length of Array that will be returned"}),"\n",(0,i.jsx)(n.h6,{id:"returns-1",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,i.jsx)(n.p,{children:"Returns a single Array of size block size"}),"\n",(0,i.jsx)(n.h5,{id:"downsample",children:"downSample()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"downSample"}),"(",(0,i.jsx)(n.code,{children:"input"}),", ",(0,i.jsx)(n.code,{children:"inputRate"}),", ",(0,i.jsx)(n.code,{children:"outputRate"}),"): ",(0,i.jsx)(n.code,{children:"Float32Array"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L97",children:"SpeechConverter.ts:97"})]}),"\n",(0,i.jsx)(n.p,{children:"Takes an input Float32Array and downsamples the data to the given output rate provided"}),"\n",(0,i.jsx)(n.h6,{id:"parameters-1",children:"Parameters"}),"\n",(0,i.jsx)(n.h6,{id:"input",children:"input"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,i.jsx)(n.p,{children:"Audio sample to be downsampled"}),"\n",(0,i.jsx)(n.h6,{id:"inputrate",children:"inputRate"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"The sample rate that the audio was recorded in"}),"\n",(0,i.jsx)(n.h6,{id:"outputrate",children:"outputRate"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"The sample rate the audio is being downsampled to"}),"\n",(0,i.jsx)(n.h6,{id:"returns-2",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,i.jsx)(n.p,{children:"The down sampled data in a Float32Array object"}),"\n",(0,i.jsx)(n.h5,{id:"getloggedtext",children:"getLoggedText()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"getLoggedText"}),"(): ",(0,i.jsx)(n.a,{href:"#transcribedlogentry",children:(0,i.jsx)(n.code,{children:"transcribedLogEntry"})}),"[]"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L312",children:"SpeechConverter.ts:312"})]}),"\n",(0,i.jsx)(n.h6,{id:"returns-3",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"#transcribedlogentry",children:(0,i.jsx)(n.code,{children:"transcribedLogEntry"})}),"[]"]}),"\n",(0,i.jsx)(n.h5,{id:"getstatus",children:"getStatus()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"getStatus"}),"(): ",(0,i.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L324",children:"SpeechConverter.ts:324"})]}),"\n",(0,i.jsx)(n.p,{children:"Retrieves the current status of the Whisper model."}),"\n",(0,i.jsxs)(n.p,{children:["Returns ",(0,i.jsx)(n.code,{children:'"loading"'})," if the model has not been initialized yet,\notherwise returns the status string provided by the Whisper backend."]}),"\n",(0,i.jsx)(n.h6,{id:"returns-4",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The current operational status of the Whisper module."}),"\n"]}),"\n",(0,i.jsx)(n.h5,{id:"gettranscribed",children:"getTranscribed()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"getTranscribed"}),"(): ",(0,i.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L266",children:"SpeechConverter.ts:266"})]}),"\n",(0,i.jsx)(n.p,{children:"Retrieves the latest transcription result from the Whisper model and logs it."}),"\n",(0,i.jsx)(n.p,{children:"This method calls the underlying Whisper API to obtain the most recently\ntranscribed text. If any text has been returned from whisper, it logs it."}),"\n",(0,i.jsx)(n.h6,{id:"returns-5",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The transcribed text from the current audio chunk."}),"\n"]}),"\n",(0,i.jsx)(n.h6,{id:"throws",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,i.jsx)(n.h5,{id:"init",children:"init()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"init"}),"(",(0,i.jsx)(n.code,{children:"modelPath"}),", ",(0,i.jsx)(n.code,{children:"lang"}),"): ",(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"void"}),">"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L81",children:"SpeechConverter.ts:81"})]}),"\n",(0,i.jsx)(n.p,{children:"Initializes the Whisper module with the specified model and language."}),"\n",(0,i.jsx)(n.p,{children:"This method:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Creates the Whisper instance asynchronously."}),"\n",(0,i.jsx)(n.li,{children:"Loads the model file into the in-memory filesystem."}),"\n",(0,i.jsx)(n.li,{children:"Initializes Whisper with the model path and language code."}),"\n"]}),"\n",(0,i.jsx)(n.h6,{id:"parameters-2",children:"Parameters"}),"\n",(0,i.jsx)(n.h6,{id:"modelpath",children:"modelPath"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"Path or URL to the Whisper model file."}),"\n",(0,i.jsx)(n.h6,{id:"lang",children:"lang"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"Language code (e.g., 'en') to configure the model."}),"\n",(0,i.jsx)(n.h6,{id:"returns-6",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"void"}),">"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Resolves when the Whisper module is fully initialized."}),"\n"]}),"\n",(0,i.jsx)(n.h5,{id:"loadmodeltofs",children:"loadModelToFS()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"loadModelToFS"}),"(",(0,i.jsx)(n.code,{children:"modelPath"}),"): ",(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"string"}),">"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L50",children:"SpeechConverter.ts:50"})]}),"\n",(0,i.jsx)(n.p,{children:"Loads a Whisper model from a given path (local or remote) into the in-memory file system."}),"\n",(0,i.jsx)(n.p,{children:"Fetches the model file, writes it into the MEMFS, and returns the internal path\nwhere the model is stored for later use by the Whisper instance."}),"\n",(0,i.jsx)(n.h6,{id:"parameters-3",children:"Parameters"}),"\n",(0,i.jsx)(n.h6,{id:"modelpath-1",children:"modelPath"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"The path or URL to the model file to load."}),"\n",(0,i.jsx)(n.h6,{id:"returns-7",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"string"}),">"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The internal file path in MEMFS where the model is stored."}),"\n"]}),"\n",(0,i.jsx)(n.h6,{id:"throws-1",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module is not initialized or fetch fails."}),"\n",(0,i.jsx)(n.h5,{id:"logtext",children:"logText()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"logText"}),"(",(0,i.jsx)(n.code,{children:"text"}),"): ",(0,i.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L298",children:"SpeechConverter.ts:298"})]}),"\n",(0,i.jsx)(n.p,{children:"Takes any recognized words from whisper and logs them into an array that contains a timestamp\nExcludes the string returned from whisper [BLANK_AUDIO]"}),"\n",(0,i.jsx)(n.h6,{id:"parameters-4",children:"Parameters"}),"\n",(0,i.jsx)(n.h6,{id:"text",children:"text"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"transcribed words that have been recognized by whisper"}),"\n",(0,i.jsx)(n.h6,{id:"returns-8",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"void"})}),"\n",(0,i.jsx)(n.h5,{id:"processtext",children:"processText()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"processText"}),"(",(0,i.jsx)(n.code,{children:"text"}),"): ",(0,i.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L285",children:"SpeechConverter.ts:285"})]}),"\n",(0,i.jsx)(n.p,{children:"Takes in text and calls CommandConverter for processing and\ncommand matching."}),"\n",(0,i.jsx)(n.h6,{id:"parameters-5",children:"Parameters"}),"\n",(0,i.jsx)(n.h6,{id:"text-1",children:"text"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"string"})}),"\n",(0,i.jsx)(n.p,{children:"transcribed words that have been recognized by whisper"}),"\n",(0,i.jsx)(n.h6,{id:"returns-9",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"void"})}),"\n",(0,i.jsx)(n.h5,{id:"setaudio",children:"setAudio()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"private"})," ",(0,i.jsx)(n.strong,{children:"setAudio"}),"(",(0,i.jsx)(n.code,{children:"index"}),", ",(0,i.jsx)(n.code,{children:"audio"}),"): ",(0,i.jsx)(n.code,{children:"number"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L249",children:"SpeechConverter.ts:249"})]}),"\n",(0,i.jsx)(n.p,{children:"Sets the audio data at a given index for the Whisper model."}),"\n",(0,i.jsx)(n.p,{children:"This method passes a Float32Array of audio samples\ndirectly to the Whisper backend for processing."}),"\n",(0,i.jsx)(n.h6,{id:"parameters-6",children:"Parameters"}),"\n",(0,i.jsx)(n.h6,{id:"index",children:"index"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsx)(n.p,{children:"The index position of the audio buffer to set (usually 0 or 1 for channels)."}),"\n",(0,i.jsx)(n.h6,{id:"audio",children:"audio"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Float32Array"})}),"\n",(0,i.jsx)(n.p,{children:"The raw audio data to be sent to the Whisper model."}),"\n",(0,i.jsx)(n.h6,{id:"returns-10",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"number"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The status code or result returned by the Whisper backend."}),"\n"]}),"\n",(0,i.jsx)(n.h6,{id:"throws-2",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,i.jsx)(n.h5,{id:"startlistening",children:"startListening()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"startListening"}),"(): ",(0,i.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L173",children:"SpeechConverter.ts:173"})]}),"\n",(0,i.jsx)(n.p,{children:"Starts listening to the user's microphone input, collects audio chunks,\nand feeds them into the Whisper model for transcription in real time."}),"\n",(0,i.jsxs)(n.p,{children:["The method continuously gathers small chunks from ",(0,i.jsx)(n.code,{children:"AudioInputHandler"}),",\ncombines them into fixed-size blocks, downsamples them to 16kHz (required by Whisper),\nand sends them to the model for inference."]}),"\n",(0,i.jsx)(n.h6,{id:"returns-11",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"void"})}),"\n",(0,i.jsx)(n.h6,{id:"throws-3",children:"Throws"}),"\n",(0,i.jsxs)(n.p,{children:["Throws if ",(0,i.jsx)(n.code,{children:"init()"})," was not called before invoking this method."]}),"\n",(0,i.jsx)(n.h5,{id:"stoplistening",children:"stopListening()"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"stopListening"}),"(): ",(0,i.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L229",children:"SpeechConverter.ts:229"})]}),"\n",(0,i.jsx)(n.p,{children:"Stops the audio input stream and halts the real-time transcription process."}),"\n",(0,i.jsxs)(n.p,{children:["This should be called after ",(0,i.jsx)(n.code,{children:"startListening()"})," to stop capturing microphone input\nand free up system audio resources."]}),"\n",(0,i.jsx)(n.h6,{id:"returns-12",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"void"})}),"\n",(0,i.jsx)(n.h6,{id:"throws-4",children:"Throws"}),"\n",(0,i.jsx)(n.p,{children:"Throws if the Whisper module has not been initialized."}),"\n",(0,i.jsx)(n.h2,{id:"interfaces",children:"Interfaces"}),"\n",(0,i.jsx)(n.h3,{id:"transcribedlogentry",children:"transcribedLogEntry"}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L9",children:"SpeechConverter.ts:9"})]}),"\n",(0,i.jsx)(n.p,{children:"Represents a single transcription log entry.\nEach entry contains the transcribed text and the time it was captured."}),"\n",(0,i.jsx)(n.h4,{id:"properties-1",children:"Properties"}),"\n",(0,i.jsx)(n.h5,{id:"timestamp",children:"timestamp"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"timestamp"}),": ",(0,i.jsx)(n.code,{children:"Date"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L11",children:"SpeechConverter.ts:11"})]}),"\n",(0,i.jsx)(n.p,{children:"The timestamp indicating when the transcription occurred."}),"\n",(0,i.jsx)(n.h5,{id:"transcribedtext",children:"transcribedText"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"transcribedText"}),": ",(0,i.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/blob/4f45f27607314e244cc3e3d47559464b42ec5a6c/src/SpeechConverter.ts#L13",children:"SpeechConverter.ts:13"})]}),"\n",(0,i.jsx)(n.p,{children:"The text that was transcribed at the given timestamp."})]})}function a(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);