"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[3153],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var s=i(96540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},63592:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"system-architecture/class-diagram","title":"Class Diagram","description":"This diagram illustrates the core architecture of AAC Voice API, showing the relationships between main classes","source":"@site/docs/system-architecture/class-diagram.md","sourceDirName":"system-architecture","slug":"/system-architecture/class-diagram","permalink":"/project-001-aac-api/docs/system-architecture/class-diagram","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-001-aac-api/edit/main/documentation/docs/system-architecture/class-diagram.md","tags":[],"version":"current","lastUpdatedBy":"Michael Colbert","frontMatter":{"title":"Class Diagram"},"sidebar":"docsSidebar","previous":{"title":"Version Control","permalink":"/project-001-aac-api/docs/system-architecture/version-control"},"next":{"title":"API Specification","permalink":"/project-001-aac-api/docs/category/api-specification"}}');var t=i(74848),a=i(28453);const o={title:"Class Diagram"},r="Class Diagram",c={},l=[{value:"AACVoiceApi",id:"aacvoiceapi",level:3},{value:"AudioHandlerInput",id:"audiohandlerinput",level:3},{value:"SpeechToText",id:"speechtotext",level:3},{value:"CommandMapper",id:"commandmapper",level:3},{value:"SpeechSeperation (Still in Discussion)",id:"speechseperation-still-in-discussion",level:3},{value:"Key Relationships",id:"key-relationships",level:3}];function d(e){const n={h1:"h1",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"class-diagram",children:"Class Diagram"})}),"\n",(0,t.jsx)(n.p,{children:"This diagram illustrates the core architecture of AAC Voice API, showing the relationships between main classes\nand their responsibilites"}),"\n",(0,t.jsx)(n.mermaid,{value:"classDiagram\n\n    class index {\n        +AACVoiceApi: AACVoiceApi\n    }\n\n    class AACVoiceApi {\n        +AACVoiceApi.startListening()\n        +AACVoiceApi.initalize(voiceCommands: Object)\n    }\n\n    class AudioHandlerInput {\n        -stream: MediaStream\n        -ctx: AudioContext\n        -processor: ScriptProcessorNode\n        +isListening: boolean\n        -onAudioChunk: (chunk: Float32Array) => void\n        +constructor(onAudioChunk)\n        +onAudioChunk() \n        +startListening(): Promise<void>\n        +stopListening(): void\n    }\n\n    class SpeechToText {\n        \n    }\n\n    class CommandMapper {\n       \n    }\n\n    class SpeechSeperation {\n       \n    }\n\n    AACVoiceApi *-- AudioHandlerInput : contains\n    AACVoiceApi *-- SpeechToText : contains\n    AACVoiceApi *-- CommandMapper : contains\n    index        <--  AACVoiceApi\n    AudioHandlerInput *-- SpeechSeperation : contains"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Figure 2"})," Class Architecture of our Api"]}),"\n",(0,t.jsx)(n.h3,{id:"aacvoiceapi",children:"AACVoiceApi"}),"\n",(0,t.jsx)(n.p,{children:"The main entry point for the API. This will be a facade and the only class that a developer will have to initialize.\nInitializes voice command recognition and manages the lifeycle of voice listening sessions."}),"\n",(0,t.jsx)(n.h3,{id:"audiohandlerinput",children:"AudioHandlerInput"}),"\n",(0,t.jsx)(n.p,{children:"Handles raw audio stream processing from the users microphone. Manages Web Audio API components and converts audio into processable chunks."}),"\n",(0,t.jsx)(n.h3,{id:"speechtotext",children:"SpeechToText"}),"\n",(0,t.jsx)(n.p,{children:"Consumes (from AudioHandlerInput) and converts audio chunks into text transcriptions for command recognition."}),"\n",(0,t.jsx)(n.h3,{id:"commandmapper",children:"CommandMapper"}),"\n",(0,t.jsx)(n.p,{children:"Maps recognized speech text to configured voice commands and triggers the appropriate functions"}),"\n",(0,t.jsx)(n.h3,{id:"speechseperation-still-in-discussion",children:"SpeechSeperation (Still in Discussion)"}),"\n",(0,t.jsx)(n.p,{children:"Processes audio to separate speech from background noise, improving recognition accuracy"}),"\n",(0,t.jsx)(n.h3,{id:"key-relationships",children:"Key Relationships"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"AACVoiceApi serves as the orchestrator, containing and coordinating all major components"}),"\n",(0,t.jsx)(n.li,{children:"AudioHandlerInput captures and preprocessors audio before passing it to the speech recognition pipeline"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);